# 基礎理論
## 1.1 集合と論理
### 1.1.1 集合論理
集合とは、ある条件を見たし、他のもとは明確に区別できるものの集まり<br>
#### 部分集合とべき集合
ある一つの集合$U$について、それに属するものを**要素**といい、要素数が0である集合を**空集合$Φ$**と表す<br>
また、要素が有限個の場合、**有限集合**、無限個の場合**無限集合**という。さらに、1つの集合の中からいくつかの集合を考えることもでき、これを**部分集合**という。部分集合は空集合$Φ$と集合$U$自身を含めて、$2^{要素数}$の個数が考えられ、この部分集合を要素としてた集合の集合のことを**べき集合**という

|例||
|---|---|
|集合$U$|{1,2,3}|
|空集合$Φ$|{ }|
|Uの部分集合|$Φ$,{1},{2},{3},{1,2},{2,3},{1,3},{1,2,3}|
|べき集合|{$Φ$,{1},{2},{3},{1,2},{2,3},{1,3},{1,2,3}}|

#### 差集合と対象差
ある集合AとBがある時、集合Aではあるが集合Bではない要素の集合をAとBの**差集合**といい、$A - B$で表す。また、どちらか片方の要素でしかない要素の集合を**対象差**といい、$AΔB$と表す。<br>
![](images/1_%E5%B7%AE%E9%9B%86%E5%90%88.png)
![](images/1_%E5%AF%BE%E8%B1%A1%E5%B7%AE.png)

#### 集合の要素数
集合$A$の要素数をは一般的に$n(A)$と表し、集合A,B,Cがそれぞれ有限集合であれば、和集合の要素数は以下の公式で求める事が可能

1. $n(A \cup B) = n(A) + n(B) - n(A \cap B)$
2. $n(A \cup B \cup C) = n(A) + n(B) + n(C) - n(A \cap B) - n(B \cap C) - n(A \cap C) + n(A \cap B \cap C)$

例えば以下の図の場合<br>
$n(A) = 66,n(B) = 54,n(\overline{A \cup B}) = 6,n(A \cup B) = 100 - n(\overline{A \cup B}) = 94$<br>
![](images/1_%E5%92%8C%E9%9B%86%E5%90%88.png)

### 1.1.2 命題と論理
#### 命題
**命題**とは1つの判断や主張を記号や文章で表したもので、trueかfalseで区別できるものを指す<br>
例えば、集合U={1,2,3}における任意の部分集合をX,Yとして、このX,Yについて「$X \cap Y=X である$」という主張は$X=\{1\},Y=\{1,2\}$の時は真だが,$X=\{1,2\},Y=\{1\}$の場合は偽となる<br>
このように命題にある変数xを含み、xの値によって真偽が決まる命題を**条件命題(命題関数)**と呼び、$p(x),q(x)$というように表す<br>

#### 複合命題
複数の命題を「かつ」、「又は」などで結んで作られる命題を**複合命題(合成命題)**という
||意味|論理記号|真理値|
|---|---|---|---|
|連合命題|$pかつq$|$p\land q$|p=1,q=1の時真、それ以外は偽|
|選言命題|$pまたはq$|$p\lor q$|p=0,q=0の時偽、それ以外は真|
|否定命題|$pではない$|$\neg p$|p=0の時真、そうでなければ偽|
|条件文|$pならばq$|$p\to q$|p=1が真でqが偽の時、偽。それ以外は真|
|総条件文|$pならばq　かつ　qならばp$|$p \leftrightarrow q$|p=1,q=1の真理値が同じとき真|

### 条件文
[pならばq]という命題を**条件文**といい、$p\to q$と表し、「真⇒偽」となる時に限り、偽となる演算でこれを**含意**という
$p\to q$の真理値表を考える時は、"pであってqではないことはない"と考える。つまり$p\to q$の真理値は論理式$\neg (p \land \neg q)$と同値になる<br>
この式をド・モルガンの法則を用いて展開すると$\neg p \lor q$と表す事も可能

![](images/1_%E5%90%AB%E6%84%8F.png)

この真理値表を見ると、pが真でqが偽の時に限り、$\to$は偽となる演算だとわかる

#### 条件文の逆・裏・対偶
ある条件文$p \to q$の逆・裏・対偶は以下のようになる.
もとの条件文と対偶の真偽は一致する

![](images/1_%E9%80%86%EF%BC%BF%E8%A3%8F%EF%BC%BF%E5%AF%BE%E5%81%B6.png)

この性質を利用して以下の前提条件から論理的に導くことができる結論はどちらであるかを考えてみる

>[前提条件]<br>
>受験生は毎朝紅茶かコーヒーのどちらかを飲み、両方飲むことはない。紅茶を飲むときは必ずサンドイッチを食べ、コーヒーを飲むときは必ずトーストを食べる<br>
>[結論]<br>
>①受験生は朝、サンドイッチを食べるときは紅茶を飲む<br>
>②受験生は朝、サンドイッチを食べないならばコーヒーを飲む

①の条件は「紅茶を飲むならば、サンドイッチを食べる」の逆命題であるため、真理値表が一致するとは限らない(サンドイッチとトーストを両方食べる時はないという記述がないため、サンドイッチを食べたからといって紅茶を飲んでいるとは限らない)<br>
②の条件は「紅茶を飲むならば、サンドイッチを食べる」の対偶命題である「サンドイッチを食べないならば、紅茶を飲まない」との真理値が同じであることを考えて、この時紅茶かコーヒーのどちらかを飲むという条件を考慮すると「サンドイッチを食べないならば、コーヒーを飲む」と導く事ができる

### 1.1.3 論理演算
#### 論理演算と集合演算
論理積(AND)、論理和(OR)、否定演算(NOT)があり、この3津の演算を組み合わせることにより、様々な論理回路作る事が可能

演算則は以下の通りである
![](images/1_%E6%BC%94%E7%AE%97%E6%B8%AC.png)

### 1.1.4 論理式の簡略化
### 演算則を用いた方法
例) $\overline{A} \cdot \overline{B} + \overline{A} \cdot B + A \cdot \overline{B}$<br>
$\overline{A} \cdot \overline{B} + \overline{A} \cdot B + A \cdot \overline{B}$<br>
$= \overline{A} + A \cdot \overline{B} + \overline{A} \cdot \overline{B}$　べき等則により同じモノを足しても値は変わらない<br>
$= \overline{A} + \overline{B}$ <br>
$= \overline{A \cdot B}$　ドモルガンの法則<br>

###　カルノー図を用いた簡略化
論理式を図的に表現したモノが**カルノー図**といい、それを用いると視覚的に論理式を整理することが可能<br>
先ほどの例でいうと、
|A\B|0|1|
|---|---|---|
|0|1|1|
|1|1|0|

この例の場合、Aが0の時Bの値に関わらず1になることと、Bが0の時にAの値に関わらず1になることが分かる。
故に$\overline{A} + \overline{B}$ という風に簡略化が可能

## 1.2 情報理論と符号化
### 1.2.1　情報量
#### 情報量（I)
一般に**情報量**は情報の大きさを定量化した値で表す。具体的にはある事象$J$が起こった時に伝達される情報の大きさを次の式で表す<br>
$I(J) = -\log_2 P(J)$

事象の生起確率が大きくなるほど、情報量は小さくなり、事象の生起確率が大きいほど情報量が大きくなる

#### 情報の加法性
ある事象$J_a$と$J_b$が互いに独立であるとき、同時に起こった時の情報量はそれぞれの情報量の和で表すことができる<br>
$I(J_ab) = I(J_a) + I(J_b) = -\log_2 P(J_a) - \log_2 P(J_b) = -\log_2 P(J_ab)$

#### 平均情報量(エントロピー)
全ての事象($J_1,...,J_n$)の平均的な情報量を**平均情報量$H$(エントロピー)**という

$H = \sum_{i=1}^{n}P(J_i) \times I(J_i)$

平均情報量は曖昧さの程度を示しており、平均情報量が小さい程、曖昧さが少なく、事象の予測がしやすい1<br>
例)天気予報
- 50%で晴れ、50%で雨の時の平均情報量 = 1[ビット]
- 100%で晴れる時の平均情報量 = 0[ビット]

となり100%の時は曖昧さがないことが分かる

### 1.2.2 情報源符号化
情報を正しく伝達するための**通信路符号化**や情報が膨大であるときにできるだけ小さくする**情報源符号化**を行う事で情報は正しく、効率よく受信者に伝達される

#### ハフマン符号化
**ハフマン符号化**は出現頻度の高い文字を短いビット列で表現し、出現頻度の低い文字程長いビット列で符号化することにより、平均符号長を最小にする情報の圧縮手法

例) 以下の出現頻度の文字がある場合
|文字|出現確率|
|---|---|
|a|0.5|
|b|0.3|
|c|0.1|
|d|0.1|

文字が四つなので、2ビットを用いてそのまま符号化した時の平均符号長は2ビットとなる。<br>
これに対してハフマン符号化を行う。ハフマン符号化は**ハフマン木**を用いて以下の手順で行われる<br>
1. 文字の中で最小の出現確率の文字を二つ選び、大きい方に0,小さい法に1を割り当て、一つの木にまとめ、重みを二つの要素の和とする
2. 木の出現確率の大きい順に並び変える
3. 2を最終的に一つの木にまとまるまで続ける

そうすると以下のようなハフマン木が生成される。
![](images/1_%E3%83%8F%E3%83%95%E3%83%9E%E3%83%B3%E6%9C%A8.png)

これにより各文字に割り当てた符号を用いて、同様に平均符号長を計算すると<br>
$0.5 * 1 + 0.3 * 2 + 0.1 * 3 + 0.1 * 3 = 1.7[ビット]$<br>
となり、普通に2ビット割り当てるよりも少ない符号長で表現することが可能

### ラングレンス符号化
データ列の冗長度に注目して、同じデータ値が連続する部分をその反復回数とデータ値に置き換えることで、データ長を短くする手法を**ラングレンス符号化**という

![](images/1_%E3%83%A9%E3%83%B3%E3%82%B0%E3%83%AC%E3%83%B3%E3%82%B9%E7%AC%A6%E5%8F%B7%E5%8C%96.png)

ラングレンス符号化は文字データだけでなく、二値画像の圧縮などにも使用されており、白を「0」と黒を「1」とすると「00000000000111111111100」は「12,10,2」と表す事ができる

### 1.2.3 ディジタル符号化
アナログデータをディジタル符号に変換する**パルス符号変調PCM**について説明する。
PCMは以下の3手順で行われる
1. 標本化
   - アナログ信号を一定間隔で切り出す。
   - 1秒間にサンプリングする回数を**サンプリング周波数**という  
2. 量子化
   - サンプリングしたアナログ値をディジタル値に変換する。
   - 1回のサンプリングで生成されるビット数を量子化ビット数といい、量子化ビット数が8ビットであれば0~255の階調の数値に変換される 
3. 符号化
   - 量子化で得られたディジタル信号を2進数に変換して符号化ビット列を得る

 
