# 基礎理論
## 1.1 集合と論理
### 1.1.1 集合論理
集合とは、ある条件を見たし、他のもとは明確に区別できるものの集まり<br>
#### 部分集合とべき集合
ある一つの集合$U$について、それに属するものを**要素**といい、要素数が0である集合を**空集合$Φ$**と表す<br>
また、要素が有限個の場合、**有限集合**、無限個の場合**無限集合**という。さらに、1つの集合の中からいくつかの集合を考えることもでき、これを**部分集合**という。部分集合は空集合$Φ$と集合$U$自身を含めて、$2^{要素数}$の個数が考えられ、この部分集合を要素としてた集合の集合のことを**べき集合**という

|例||
|---|---|
|集合$U$|{1,2,3}|
|空集合$Φ$|{ }|
|Uの部分集合|$Φ$,{1},{2},{3},{1,2},{2,3},{1,3},{1,2,3}|
|べき集合|{$Φ$,{1},{2},{3},{1,2},{2,3},{1,3},{1,2,3}}|

#### 差集合と対象差
ある集合AとBがある時、集合Aではあるが集合Bではない要素の集合をAとBの**差集合**といい、$A - B$で表す。また、どちらか片方の要素でしかない要素の集合を**対象差**といい、$AΔB$と表す。<br>
![](images/1_%E5%B7%AE%E9%9B%86%E5%90%88.png)
![](images/1_%E5%AF%BE%E8%B1%A1%E5%B7%AE.png)

#### 集合の要素数
集合$A$の要素数をは一般的に$n(A)$と表し、集合A,B,Cがそれぞれ有限集合であれば、和集合の要素数は以下の公式で求める事が可能

1. $n(A \cup B) = n(A) + n(B) - n(A \cap B)$
2. $n(A \cup B \cup C) = n(A) + n(B) + n(C) - n(A \cap B) - n(B \cap C) - n(A \cap C) + n(A \cap B \cap C)$

例えば以下の図の場合<br>
$n(A) = 66,n(B) = 54,n(\overline{A \cup B}) = 6,n(A \cup B) = 100 - n(\overline{A \cup B}) = 94$<br>
![](images/1_%E5%92%8C%E9%9B%86%E5%90%88.png)

### 1.1.2 命題と論理
#### 命題
**命題**とは1つの判断や主張を記号や文章で表したもので、trueかfalseで区別できるものを指す<br>
例えば、集合U={1,2,3}における任意の部分集合をX,Yとして、このX,Yについて「$X \cap Y=X である$」という主張は$X=\{1\},Y=\{1,2\}$の時は真だが,$X=\{1,2\},Y=\{1\}$の場合は偽となる<br>
このように命題にある変数xを含み、xの値によって真偽が決まる命題を**条件命題(命題関数)**と呼び、$p(x),q(x)$というように表す<br>

#### 複合命題
複数の命題を「かつ」、「又は」などで結んで作られる命題を**複合命題(合成命題)**という
||意味|論理記号|真理値|
|---|---|---|---|
|連合命題|$pかつq$|$p\land q$|p=1,q=1の時真、それ以外は偽|
|選言命題|$pまたはq$|$p\lor q$|p=0,q=0の時偽、それ以外は真|
|否定命題|$pではない$|$\neg p$|p=0の時真、そうでなければ偽|
|条件文|$pならばq$|$p\to q$|p=1が真でqが偽の時、偽。それ以外は真|
|総条件文|$pならばq　かつ　qならばp$|$p \leftrightarrow q$|p=1,q=1の真理値が同じとき真|

### 条件文
[pならばq]という命題を**条件文**といい、$p\to q$と表し、「真⇒偽」となる時に限り、偽となる演算でこれを**含意**という
$p\to q$の真理値表を考える時は、"pであってqではないことはない"と考える。つまり$p\to q$の真理値は論理式$\neg (p \land \neg q)$と同値になる<br>
この式をド・モルガンの法則を用いて展開すると$\neg p \lor q$と表す事も可能

![](images/1_%E5%90%AB%E6%84%8F.png)

この真理値表を見ると、pが真でqが偽の時に限り、$\to$は偽となる演算だとわかる

#### 条件文の逆・裏・対偶
ある条件文$p \to q$の逆・裏・対偶は以下のようになる.
もとの条件文と対偶の真偽は一致する

![](images/1_%E9%80%86%EF%BC%BF%E8%A3%8F%EF%BC%BF%E5%AF%BE%E5%81%B6.png)

この性質を利用して以下の前提条件から論理的に導くことができる結論はどちらであるかを考えてみる

>[前提条件]<br>
>受験生は毎朝紅茶かコーヒーのどちらかを飲み、両方飲むことはない。紅茶を飲むときは必ずサンドイッチを食べ、コーヒーを飲むときは必ずトーストを食べる<br>
>[結論]<br>
>①受験生は朝、サンドイッチを食べるときは紅茶を飲む<br>
>②受験生は朝、サンドイッチを食べないならばコーヒーを飲む

①の条件は「紅茶を飲むならば、サンドイッチを食べる」の逆命題であるため、真理値表が一致するとは限らない(サンドイッチとトーストを両方食べる時はないという記述がないため、サンドイッチを食べたからといって紅茶を飲んでいるとは限らない)<br>
②の条件は「紅茶を飲むならば、サンドイッチを食べる」の対偶命題である「サンドイッチを食べないならば、紅茶を飲まない」との真理値が同じであることを考えて、この時紅茶かコーヒーのどちらかを飲むという条件を考慮すると「サンドイッチを食べないならば、コーヒーを飲む」と導く事ができる

### 1.1.3 論理演算
#### 論理演算と集合演算
論理積(AND)、論理和(OR)、否定演算(NOT)があり、この3津の演算を組み合わせることにより、様々な論理回路作る事が可能

演算則は以下の通りである
![](images/1_%E6%BC%94%E7%AE%97%E6%B8%AC.png)

### 1.1.4 論理式の簡略化
### 演算則を用いた方法
例) $\overline{A} \cdot \overline{B} + \overline{A} \cdot B + A \cdot \overline{B}$<br>
$\overline{A} \cdot \overline{B} + \overline{A} \cdot B + A \cdot \overline{B}$<br>
$= \overline{A} + A \cdot \overline{B} + \overline{A} \cdot \overline{B}$　べき等則により同じモノを足しても値は変わらない<br>
$= \overline{A} + \overline{B}$ <br>
$= \overline{A \cdot B}$　ドモルガンの法則<br>

###　カルノー図を用いた簡略化
論理式を図的に表現したモノが**カルノー図**といい、それを用いると視覚的に論理式を整理することが可能<br>
先ほどの例でいうと、
|A\B|0|1|
|---|---|---|
|0|1|1|
|1|1|0|

この例の場合、Aが0の時Bの値に関わらず1になることと、Bが0の時にAの値に関わらず1になることが分かる。
故に$\overline{A} + \overline{B}$ という風に簡略化が可能

## 1.2 情報理論と符号化
### 1.2.1　情報量
#### 情報量（I)
一般に**情報量**は情報の大きさを定量化した値で表す。具体的にはある事象$J$が起こった時に伝達される情報の大きさを次の式で表す<br>
$I(J) = -\log_2 P(J)$

事象の生起確率が大きくなるほど、情報量は小さくなり、事象の生起確率が大きいほど情報量が大きくなる

#### 情報の加法性
ある事象$J_a$と$J_b$が互いに独立であるとき、同時に起こった時の情報量はそれぞれの情報量の和で表すことができる<br>
$I(J_ab) = I(J_a) + I(J_b) = -\log_2 P(J_a) - \log_2 P(J_b) = -\log_2 P(J_ab)$

#### 平均情報量(エントロピー)
全ての事象($J_1,...,J_n$)の平均的な情報量を**平均情報量$H$(エントロピー)**という

$H = \sum_{i=1}^{n}P(J_i) \times I(J_i)$

平均情報量は曖昧さの程度を示しており、平均情報量が小さい程、曖昧さが少なく、事象の予測がしやすい1<br>
例)天気予報
- 50%で晴れ、50%で雨の時の平均情報量 = 1[ビット]
- 100%で晴れる時の平均情報量 = 0[ビット]

となり100%の時は曖昧さがないことが分かる

### 1.2.2 情報源符号化
情報を正しく伝達するための**通信路符号化**や情報が膨大であるときにできるだけ小さくする**情報源符号化**を行う事で情報は正しく、効率よく受信者に伝達される

#### ハフマン符号化
**ハフマン符号化**は出現頻度の高い文字を短いビット列で表現し、出現頻度の低い文字程長いビット列で符号化することにより、平均符号長を最小にする情報の圧縮手法

例) 以下の出現頻度の文字がある場合
|文字|出現確率|
|---|---|
|a|0.5|
|b|0.3|
|c|0.1|
|d|0.1|

文字が四つなので、2ビットを用いてそのまま符号化した時の平均符号長は2ビットとなる。<br>
これに対してハフマン符号化を行う。ハフマン符号化は**ハフマン木**を用いて以下の手順で行われる<br>
1. 文字の中で最小の出現確率の文字を二つ選び、大きい方に0,小さい法に1を割り当て、一つの木にまとめ、重みを二つの要素の和とする
2. 木の出現確率の大きい順に並び変える
3. 2を最終的に一つの木にまとまるまで続ける

そうすると以下のようなハフマン木が生成される。
![](images/1_%E3%83%8F%E3%83%95%E3%83%9E%E3%83%B3%E6%9C%A8.png)

これにより各文字に割り当てた符号を用いて、同様に平均符号長を計算すると<br>
$0.5 * 1 + 0.3 * 2 + 0.1 * 3 + 0.1 * 3 = 1.7[ビット]$<br>
となり、普通に2ビット割り当てるよりも少ない符号長で表現することが可能

### ラングレンス符号化
データ列の冗長度に注目して、同じデータ値が連続する部分をその反復回数とデータ値に置き換えることで、データ長を短くする手法を**ラングレンス符号化**という

![](images/1_%E3%83%A9%E3%83%B3%E3%82%B0%E3%83%AC%E3%83%B3%E3%82%B9%E7%AC%A6%E5%8F%B7%E5%8C%96.png)

ラングレンス符号化は文字データだけでなく、二値画像の圧縮などにも使用されており、白を「0」と黒を「1」とすると「00000000000111111111100」は「12,10,2」と表す事ができる

### 1.2.3 ディジタル符号化
アナログデータをディジタル符号に変換する**パルス符号変調PCM**について説明する。
PCMは以下の3手順で行われる
1. 標本化
   - アナログ信号を一定間隔で切り出す。
   - 1秒間にサンプリングする回数を**サンプリング周波数**という  
2. 量子化
   - サンプリングしたアナログ値をディジタル値に変換する。
   - 1回のサンプリングで生成されるビット数を量子化ビット数といい、量子化ビット数が8ビットであれば0~255の階調の数値に変換される 
3. 符号化
   - 量子化で得られたディジタル信号を2進数に変換して符号化ビット列を得る

PCMにもいくつか種類があり、**DPCM**は直前の標本との差分を量子化することで、データ量を削減する工夫があり、**ADPCM**標本の差分を表現するビット数をその変動幅に応じて適応的に変化させるような改良を入れている

PCMを行う時は**標本化定理**に注意する必要がある。標本化定理とは標本化の際のサンプリング周波数を元の信号の上限周波数の2倍以上に設定しなければいけないというもの定理。これに反すると、元の信号の形を復元することが出来ずに**エイリアシング**が生じる(ヘリコプターなどのプロペラの回転が逆方向に進んで見えたりしてしまうなど)

また、サンプリングの間隔のことを**サンプリング周期**といい、サンプリング周波数の逆数から求める事ができる

>例)4kHzの音声信号を量子化ビット数8ビットで量子化する時の1秒間で生成されるデータ量はバイトか

4kHzのデータを標本化する際にはサンプリング周波数を4k * 2 = 8kHzに設定する必要がある.また、一つのサンプリングデータにつき8bitの量子化を行うので、1秒間のデータ数は<br>

$8 \times 10^3 \times 8 = 6.4 \times 10^4[bit/s]$

さらに1byte = 8bitであるから、8で割ると$8.0 \times 10^3[byte/秒]$が求まる

## 1.3 オートマトン
### 1.3.1 有限オートマトン
#### 順序機械
入力値と入力された時の状態で出力地が決まるものを**順序機械**という。順序機械は、フリップフロップ回路や自動販売機のように過去の状態を保持できる回路や機械をモデル化したものであり、入力値、状態、出力値の関係を状態遷移表や図で表す事ができる

![](images/1_順序機械.png)

#### 有限オートマトン
順序機械に言語を認識するアルゴリズムを与えた数学的なモデルを**有限オートマトン**という。有限オートマトンは「有限個の状態の集合$K$、入力記号の有限集合$\Sigma$、$\Sigma$に属する各入力記号と現在の状態が引き起こす状態遷移関数$\sigma$、初期状態$q_0$、状態$K$の部分集合である受理状態集合$F$」によって定義される

例えば以下のような有限オートマトンは以下の図で表される

|||
|---|---|
|有限オ－トマトン$A$|$<K,\Sigma,\sigma,q_0,F>$|
|状態の集合K|${a,b,c,d}$|
|入力記号の有限集合$\Sigma$|$0,1$|
|状態遷移関数$\sigma$|$\sigma$は$K \times \Sigma$から$K$への写像|
|初期状態$q_0$|a|
|受理状態$F$|c|

![](images/1_%E6%9C%89%E9%99%90%E3%82%AA%E3%83%BC%E3%83%88%E3%83%9E%E3%83%88%E3%83%B3.png)

この有限オートマトンはいくつかの幅の入力ビット列を入力された時に、その入力が受理されるかを判定できる。「011０」が入力された場合「a⇒a⇒b⇒d⇒c」と遷移して最終的にcになるので受理されるが、「1001」の場合は「a⇒a⇒b⇒c⇒b」となり最後がbであるため受理されない

### 1.3.2 有限オートマトンと正規表現
**正規表現**によって表される言語を**正規言語**といい、有限オートマトン正規言語を認識するために利用される。

例えば、正規表現(0|10)*1によって表される正規言語が生成する文は最後が1で、最後の1を除く1の次は必ず0になる文であり、これらの文章は以下の有限オートマトンで認識することが可能

![](images/1_%E6%AD%A3%E8%A6%8F%E8%A1%A8%E7%8F%BE%E3%81%A8%E6%9C%89%E9%99%90%E3%82%AA%E3%83%BC%E3%83%88%E3%83%9E%E3%83%88%E3%83%B3.png)

1が2回連続で入力された場合は$Q_2$に移動するため、受理されなくなり、$Q_0$から最後に1が入力されて$Q_1$に遷移しない場合も受理されない

## 1.4 形式言語
### 1.4.1　形式文法と言語処理
コンピュータ処理のために作られた言語が**プログラミング言語**とすると、そのほとんどの構文は**文脈自由文法(形式文法)**で表すことができる

#### 文脈自由文法(形式文法)
文脈自由文法は「書き換えを行う対象となる非終端記号の集合$N$、書き換えを行う事ができない終端記号の集合$T$、書き換え規則の集合$P$、書き換えを開始する最初の非終端記号となる開始記号$S$」によって定義される

|||
|---|---|
|文脈自由文法$G$|{$N,T,P,S$}|
||$N=K,S　T=a,b　P=S⇒\epsilon,S⇒aK,K⇒bS$|

この文法では次のような分を導出できる
1. S⇒aK⇒abS⇒**ab**<br>
2. S⇒aK⇒abS⇒abaK⇒ababS⇒**abab**<br>

このように書き換え規則用いて、左辺が必ず一つの非終端記号となっているのが文脈自由文法であり、これらの文の集合を**文脈自由言語**という

### 言語処理
言語は小さい順に「文字＜字句＜文＜言語」からできており、文字から字句を生成するための規則を**字句規則**といい、字句の正しい並べ方を**構文規則**という

あるプログラムを実行する際に前もって、その文法に基づいた翻訳(コンパイル)が行われる。このコンパイル処理で行われる字句解析と構文解析を自動化するためには、字句規則と構文規則を適切に定義する必要がある。

例えば、評価順序を表すカッコを用いない四則演算からなる数式の字句規則と構文規則は次のようになる

![](images/1_%E5%9B%9B%E5%89%87%E6%BC%94%E7%AE%97%E3%81%AE%E5%AD%97%E5%8F%A5%E8%A6%8F%E5%89%87.png)

字句規則に基づいた字句の検査と切り出しを行うのが**字句解析**で字句規則は正規表現を用いて表す事が可能<br>
例えば上の数の場合[0-9]+で表すことができるので、有限オートマトンを用いて表せる

また字句解析によって切り出された字句を構文規則にしたがって解析し、文法的正当性を検査するのが**構文解析**である

### 1.4.2 構文規則の記述
#### BNF記法
BNF記法はAlgol60の構文規則を記述するのに用いられた表記法で、現在の多くのプログラミング言語の構文規則の記述に用いられている。

BNF記法では例えば1桁の数字と1文字の英字、又は1桁の英数字を以下のように定義する<br>
この場合、英数字は英字と数字いずれかの要素の内どれかが選ばれる事を示している
![](images/1_BNF1.png)

算術式の構文は以下のようになっている<br>

![](images/1_BNF2.png)

<式>の項目を見ると、<式>を定義するために、自分自身を用いる場合がある。これを**再帰的定義**という

ちなみに算術式の構文を図で表すとこのような形になる
![](images/1_BNF_structure.png)


### 1.4.3 構文解析の技法
#### 構文木
**構文解析**では構文規則に合致しているかを構文解析表を用いて解析し、同時に以下の図のような木構造で表現する。これを**構文木**という

![](images/1_構文木.png)

#### 構文木の利用
構文木から**意味解析**をするために、3つ組、4つ組、逆ポーランド記法などの手法を利用して、構文木から**中間語**を生成する

- 3つ組<br>
  構文木を下位レベルの部分木から順にまとめていく<br>
  ![](images/1_%E4%B8%89%E3%81%A4%E7%B5%84%E3%81%BF.png)

- 4つ組<br>
  3つ組の表現に演算結果をどこに代入するかの情報を加える<br>
  ![](images/1_%E5%9B%9B%E3%81%A4%E7%B5%84.png)

- 逆ポーランド記法<br>
  演算子を演算の対象である演算数の右側に記述する記法。**後置表記法**とも呼ばれる。図のようにノードをさかのぼるタイミングで操作していくことで表現することができる<br>
  以下の図の$X = 2 * 3 + 4 * 7 - 5$という式を逆ポーランド記法で記述した場合、$X23*47*+5-=$となる<br>
  ![](images/1_%E9%80%86%E3%83%9D%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%89%E8%A8%98%E6%B3%95.png)

### 1.4.4 正規表現
字句記号や探索記号の定義など、広く使用されているパターン定義法

例えばアルファベットと呼ばれる有限集合Vからいくつかの要素を取り出し、作成されたものをV上の**記号例**と呼ぶ。<br>

例) $V = {a,b,c}$<br>
$V* = a,b,c,ab,abb,abbbbc,acb...$

#### 正規表現の例
先ほどの記号列の集合V*のなかで、どの様な規則を持ったものを字句として取り扱うかを定義したモノが正規表現である。正規表現では以下の**メタ記号**を用いて表現する

|メタ記号|意味|
|---|---|
|[m-n]|m~nまでの連続した文字の中の1文字|
|*|直前の正規表現の0回以上の繰り返し|
|+|直前の正規表現の1回以上の繰り返し|
|?|直前の正規表現が0か1個ある|
|$r_1 \vert r_2$|正規表現$r_1,r_2$のいずれかであること|

例えば、$[A-Z]+[0-9]*$という正規表現があったとすると
- IKATO410
- A6240
- AB
など「英字が最低限1個以上ある後に、数字が0個以上続く字句」を表現できる

また、kimiとkamiを集合の要素とする正規表現は$kimi \vert kami$で表現できるが、これを代わりに$k(i \vert a)mi$というように短縮して表現することも可能

## 1.5 グラフ理論
### 1.5.1 有向グラフ・無向グラフ
グラフとは頂点の集合Vと頂点を連結する辺の集合Eから成り立つ図形を示す。**有向グラフ**の場合、辺に順序があり、**無向グラフ**の場合順序がない

![](images/1_%E3%82%B0%E3%83%A9%E3%83%95.png)

有向グラフにおいては、各頂点に入ってくる辺の数を**入次数**、出る辺の数を**出次数**といい、その和を**次数**といったりする

### 1.5.2 サイクリックグラフ
有向グラフの中で閉路を持つものを**サイクリックグラフ**という
![](images/1_%E3%82%B5%E3%82%A4%E3%82%AF%E3%83%AA%E3%83%83%E3%82%AF%E3%82%B0%E3%83%A9%E3%83%95.png)

非サイクリックグラフの場合、頂点番号を行き止まりまで順に並べることが可能で、これを**トポロジカル順序**という。(上の図の場合1-2-3-4)

### 1.5.3 グラフの種類
グラフの種類を以下の図にまとめる。(必要になったら覚える予定)
![](images/1_%E3%82%B0%E3%83%A9%E3%83%95%E3%81%AE%E7%A8%AE%E9%A1%9E.png)


### 1.5.4 グラフの表現
グラフの表現方法には**配列表現**と**行列表現**と**リスト表現**がある

#### 配列表現
辺の頂点 $v_i,v_j$ をそれぞれ配列に格納
![](images/1_配列表現.png)

#### 行列表現
頂点数×頂点数の行列からなる**隣接行列**という正方行列で表現する手法<br>
頂点同士が辺で繋がっている場合に対応する箇所に1を格納する<br>
頂点数が多くなると隣接行列も増加してしまうため、記憶域が膨大になるデメリットがある。
![](images/1_%E8%A1%8C%E5%88%97%E8%A1%A8%E7%8F%BE.png)

#### リスト表現
行列表現の各行を**線形リスト**で表現する手法<br>
行列表現に比べて記憶域を節約できるため、AtCoderなんかでもこちらが主に推奨される

![](images/1_%E3%83%AA%E3%82%B9%E3%83%88%E8%A1%A8%E7%8F%BE.png)

#### 行列表現の応用
隣接行列は以下の性質を持つ<br>
>隣接行列 $M^2$ は頂点 $v_i,v_j$ が一つの頂点を挟んで結ばれる経路の数、すなわち経路上の辺の数が2になる経路の数を示す

詳しい説明は省略するが、大体以下のような特徴を持つ。<br>
![](images/1_隣接行列の特徴.png)

### 1.5.3 重み付きグラフ
グラフの辺に値を持たせたグラフを**重み付きグラフ**という<br>
よく**最短経路問題**を解く際に使われる。

#### ダイクストラ法
各頂点への最小コストを、始点の隣接点から1ずつ確定し、徐々に範囲を広げていき、最終的に全ての頂点への最小コストを求める手法<br>
手順は以下の図の通り
![](images/1_%E3%83%80%E3%82%A4%E3%82%AF%E3%82%B9%E3%83%88%E3%83%A9%E6%B3%95.png)

(ダイクストラ法の解説はAtCoderの本を読んだ方がよく分かるのでそちらを参照した方が良い。優先度付きキューなどを用いた具体的な実装方法まで書いてある)

## 1.6 確率と統計
### 1.6.1 確率
#### 場合の数
場合の数とはある事象の起こり方の数のことで、サイコロの場合1~6の6種類の目が出るため、場合の数は6である

#### 組み合わせ
n個の異なるモノの中から任意にr個取ってできる組みの1つ一つを「n子からr個とってできる**組み合わせ**」と呼び $_nC_r$ で表す

組み合わせの計算式は以下の通り<br>
$_nC_r = \frac{n!}{r!(n - r)!}= \frac{n×(n-1)×・・・×(n-r+1)}{r×・・・×1}$

#### 確率の定義
起こりうるすべての場合がn通りあり、どの場合も同様に確からしく起こる時、n通りの中で事象Aが起きる場合の数がa個あるならば確率 $P(A)$ は以下の式で表される<br>
$P(A)=\frac{a}{n}$

#### 加法定理と乗法定理
事象Aと事象Bがある時、加法定理は以下のようになる

- AとBが排反である場合
  - $P(A \cup B)=P(A) + P(B)$
- AとBが排反でない場合
  - $P(A \cup B)=P(A) + P(B) - P(A \cap B)$

※排反はAが起きるときにBが起こらない事、ベン図でいうと重なっている部分がない場合がイメージしやすい

情報定理は以下のようになる
- 事象AとBが独立であるとき
  - $P(A \cap B) = P(A) \times P(B)$
- 事象Bが事象Aの従属事象であるとき
  - $P(A \cap B) = P(A) \times P_A(B)$

※独立と事象Aの結果によって事象Bの確率に変化がない場合<br>
※従属とは事象Aが起きることを条件に事象Bが起こる場合

### 1.6.2 確率の応用
#### 原因の確率
互いに排反である2つの事象$E_1,E_2$のそれぞれの結果として1つの事象Nが生じる時、Nが起こった原因が$E_1$、もしくは$E_2$である確率を**原因の確率**という。**ベイズの定理**によって次のように求めることが可能


